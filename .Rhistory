# Fit the mod_diamonds3 on diamonds_train
mod_diamond3 <- lm(lprice ~ depth + table + x + y + z + lcarat + color + cut + clarity,data=diamonds_train)
## add predictions to mod_diamond 2 and 3
model_dia_2 <- diamonds2 %>%
add_predictions(mod_diamond2, "pred2") %>% mutate(pred_price = pred2^2)
rmse(mod_diamond2,diamonds_test) # 0.1911
mod_diamond2
diamonds2
model_dia_2
sqrt(mean((model_dia_2$price - model_dia_2$pred_price)^2))
rmse(mod_diamond2,model_dia_2) # 0.1911
rmse(model_dia_2,diamonds_test) # 0.1911
model_dia_3 <- diamonds2 %>%
add_predictions(mod_diamond3, "pred3") %>% mutate(pred_price = pred3^2)
# rmse
sqrt(mean((model_dia_3$price - model_dia_3$pred_price)^2))
# random forest model on the same formula as the other trees
mod_diamond_rf <- randomForest(price ~ depth + table + x + y + z + lcarat + color + cut + clarity,
data = diamonds2, ntree = 200,
importance = TRUE, do.trace = 10)
# clarity > color > depth > cut > carat
importance(mod_diamond_rf)[order(importance(mod_diamond_rf)[,1], decreasing = TRUE),]
# random forest model on the same formula as the other trees
mod_diamond_rf <- randomForest(price ~ depth + table + x + y + z + lcarat + color + cut + clarity,
data = diamonds_train, ntree = 200,
importance = TRUE, do.trace = 10)
importance(mod_diamond_rf)[order(importance(mod_diamond_rf)[,1], decreasing = TRUE),]
# As the model gets more complicated the rmse increases because it is overfitting
rmse(regression_mod,diamonds2_test) ## 828.63
# Fit a regression tree using rpart
(regression_mod <- rpart(price ~ depth + table + x + y + z + lcarat + color + cut + clarity,data=diamonds_train))
# lcarat and y are the most important, with color being the least important
summary(regression_mod)
# As the model gets more complicated the rmse increases because it is overfitting
rmse(regression_mod,diamonds2_test) ## 828.63
# rmse
sqrt(mean((model_dia_3$price - model_dia_3$pred_price)^2))
library(tidyverse)
library(modelr)
library(rpart)
library(randomForest)
diamonds2 <- diamonds %>%
filter(carat <= 2.5) %>%
mutate(lprice = log2(price), lcarat = log2(carat))
# Splitting the data into test and train
diamonds_train_indices <- sample(1:nrow(diamonds2),0.8*nrow(diamonds2))
diamonds_train <- diamonds2 %>% slice(diamonds_train_indices)
diamonds_test <- diamonds2 %>% slice(-diamonds_train_indices)
# Fit the mod_diamonds2 on diamonds_train
mod_diamond2 <- lm(lprice ~ lcarat + color + cut + clarity, data = diamonds_train)
# Fit the mod_diamonds3 on diamonds_train
mod_diamond3 <- lm(lprice ~ depth + table + x + y + z + lcarat + color + cut + clarity,data=diamonds_train)
## add predictions to mod_diamond 2 and 3
model_dia_2 <- diamonds2 %>%
add_predictions(mod_diamond2, "pred2") %>% mutate(pred_price = pred2^2)
# rmse
sqrt(mean((model_dia_2$price - model_dia_2$pred_price)^2))
# Fit a regression tree using rpart
(regression_mod <- rpart(price ~ depth + table + x + y + z + lcarat + color + cut + clarity,data=diamonds_train))
diamonds2 <- diamonds %>%
filter(carat <= 2.5) %>%
mutate(lprice = log2(price), lcarat = log2(carat))
# 1)
# Splitting the data into test and train
diamonds_train_indices <- sample(1:nrow(diamonds2),0.8*nrow(diamonds2))
diamonds_train <- diamonds2 %>% slice(diamonds_train_indices)
diamonds_test <- diamonds2 %>% slice(-diamonds_train_indices)
# Fit the mod_diamonds2 on diamonds_train
mod_diamond2 <- lm(lprice ~ lcarat + color + cut + clarity, data = diamonds_train)
# Fit the mod_diamonds3 on diamonds_train
mod_diamond3 <- lm(lprice ~ depth + table + x + y + z + lcarat + color + cut + clarity,data=diamonds_train)
## add predictions to mod_diamond 2 and 3
model_dia_2 <- diamonds2 %>%
add_predictions(mod_diamond2, "pred2") %>% mutate(pred_price = pred2^2)
# rmse
sqrt(mean((model_dia_2$price - model_dia_2$pred_price)^2))
model_dia_3 <- diamonds2 %>%
add_predictions(mod_diamond3, "pred3") %>% mutate(pred_price = pred3^2)
# rmse
sqrt(mean((model_dia_3$price - model_dia_3$pred_price)^2))
# 2)
# Fit a regression tree using rpart
(regression_mod <- rpart(price ~ depth + table + x + y + z + lcarat + color + cut + clarity,data=diamonds_train))
# lcarat and y are the most important, with color being the least important
summary(regression_mod)
# As the model gets more complicated the rmse increases because it is overfitting
rmse(regression_mod,diamonds2_test) ## ~ 828.63
# 3)
# As the model gets more complicated the rmse increases because it is overfitting
rmse(regression_mod,diamonds_test) ## ~ 828.63
library(readr)
library(dplyr)
library(ggplot2)
library(tidyverse)
library(stringr)
library(ggrepel)
setwd("~/Documents/Github/UberDataAnalysis")
IMDB <- read.csv("IMDM_ratings.csv")
sum(duplicated(IMDB))
imdb <- IMDB[!duplicated(IMDB), ]
imdb <- IMDB[!duplicated(IMDB), ]
str(imdb)
# 5043 rows abd 28 cols
dim(imdb)
# What are the missing values?
colSums(sapply(imdb, is.na))
# cleaning the movie title
imdb$movie_title <- gsub("Â", "", as.character(factor(imdb$movie_title)))
str_trim(imdb$movie_title, side = "right")
imdb <- imdb[!is.na(imdb$gross), ]
imdb <- imdb[!is.na(imdb$budget), ]
# Aspect Ratio has some missing values, which will not be important for our analysis.
imdb <- subset(imdb, select = -c(aspect_ratio))
imdb <- subset(imdb, select = -c(color))
## U.K , France and Canada had highest IMDB Average Rating
# Is language in this dataset?
table(imdb$language)
# Since most movies are in English, we can remove language.
imdb <- subset(imdb, select = -c(language))
# Adding Profit and ROI %
imdb <- imdb %>% mutate(net_profit = gross - budget,return_on_investment = (net_profit/budget)*100)
acenumber_in_poster
# replacing all NA's with col average
imdb$facenumber_in_poster[is.na(imdb$facenumber_in_poster)] <- round(mean(imdb$facenumber_in_poster, na.rm = TRUE))
# replacing all NA's with col average
imdb$facenumber_in_poster[is.na(imdb$facenumber_in_poster)] <- round(mean(imdb$facenumber_in_poster, na.rm = TRUE))
# replacing all 0's with NA's
imdb[,c(5,6,8,13,24,26)][imdb[,c(5,6,8,13,24,26)] == 0] <- NA
str(imdb)
imdb$num_critic_for_reviews[is.na(imdb$num_critic_for_reviews)] <- round(mean(imdb$num_critic_for_reviews, na.rm = TRUE))
imdb$duration[is.na(imdb$duration)] <- round(mean(imdb$duration, na.rm = TRUE))
imdb$director_facebook_likes[is.na(imdb$director_facebook_likes)] <- round(mean(imdb$director_facebook_likes, na.rm = TRUE))
imdb$actor_3_facebook_likes[is.na(imdb$actor_3_facebook_likes)] <- round(mean(imdb$actor_3_facebook_likes, na.rm = TRUE))
imdb$actor_1_facebook_likes[is.na(imdb$actor_1_facebook_likes)] <- round(mean(imdb$actor_1_facebook_likes, na.rm = TRUE))
imdb$cast_total_facebook_likes[is.na(imdb$cast_total_facebook_likes)] <- round(mean(imdb$cast_total_facebook_likes, na.rm = TRUE))
imdb$actor_2_facebook_likes[is.na(imdb$actor_2_facebook_likes)] <- round(mean(imdb$actor_2_facebook_likes, na.rm = TRUE))
imdb$movie_facebook_likes[is.na(imdb$movie_facebook_likes)] <- round(mean(imdb$movie_facebook_likes, na.rm = TRUE))
view(imdb)
imdb$content_rating[imdb$content_rating == 'M']   <- 'PG'
imdb$content_rating[imdb$content_rating == 'GP']  <- 'PG'
imdb$content_rating[imdb$content_rating == 'X']   <- 'NC-17'
imdb$content_rating[imdb$content_rating == 'Approved']  <- 'R'
imdb$content_rating[imdb$content_rating == 'Not Rated'] <- 'R'
imdb$content_rating[imdb$content_rating == 'Passed']    <- 'R'
imdb$content_rating[imdb$content_rating == 'Unrated']   <- 'R'
imdb$content_rating <- factor(imdb$content_rating)
levels(imdb$country) <- c(levels(imdb$country), "Others")
imdb$country[(imdb$country != 'USA')&(imdb$country != 'UK')] <- 'Others'
imdb$country <- factor(imdb$country)
table(imdb$country)
imdb %>%
filter(title_year %in% c(2000:2016)) %>%
arrange(desc(net_profit)) %>%
top_n(10, net_profit) %>%
ggplot(aes(x=budget/1000000, y=net_profit/1000000)) +
geom_point() +
geom_smooth() +
geom_text_repel(aes(label=movie_title)) +
labs(x = "Budget in million", y = "Profit in million", title = "Budget of the top 10 profitable movies between 2000 - 2016") +
theme(plot.title = element_text(hjust = 0.2))
ggplot(data = melt(imdb), mapping = aes(x = value)) +
geom_histogram(bins = 10) + facet_wrap(~variable, scales = 'free_x')
library(reshape2)
library(corrgram)
# facet wrap
install.packages("reshape2")
# facet wrap
install.packages("corrgram")
library(reshape2)
library(corrgram)
ggplot(data = melt(imdb), mapping = aes(x = value)) +
geom_histogram(bins = 10) + facet_wrap(~variable, scales = 'free_x')
ggplot(imdb, aes(x= imdb_score)) + geom_bar()
library(readr)
library(dplyr)
library(ggplot2)
library(tidyverse)
library(stringr)
library(ggrepel)
# What variable affects the higher IMDB score?
## Predicting new user booking.
IMDB <- read.csv("IMDM_ratings.csv")
sum(duplicated(IMDB))
imdb <- IMDB[!duplicated(IMDB), ]
str(imdb)
# 5043 rows abd 28 cols
dim(imdb)
# What are the missing values?
colSums(sapply(imdb, is.na))
# cleaning the movie title
imdb$movie_title <- gsub("Â", "", as.character(factor(imdb$movie_title)))
str_trim(imdb$movie_title, side = "right")
# Gross and Budget have many missing NA's as we would use these rows.
# We will remove the rows with NA's to make it optimal.
imdb <- imdb[!is.na(imdb$gross), ]
imdb <- imdb[!is.na(imdb$budget), ]
# Aspect Ratio has some missing values, which will not be important for our analysis.
imdb <- subset(imdb, select = -c(aspect_ratio))
imdb <- subset(imdb, select = -c(color))
## U.K , France and Canada had highest IMDB Average Rating
# Is language in this dataset?
table(imdb$language)
# Since most movies are in English, we can remove language.
imdb <- subset(imdb, select = -c(language))
# Adding Profit and ROI %
imdb <- imdb %>% mutate(net_profit = gross - budget,return_on_investment = (net_profit/budget)*100)
# replacing all NA's with col average
imdb$facenumber_in_poster[is.na(imdb$facenumber_in_poster)] <- round(mean(imdb$facenumber_in_poster, na.rm = TRUE))
# replacing all 0's with NA's
imdb[,c(5,6,8,13,24,26)][imdb[,c(5,6,8,13,24,26)] == 0] <- NA
# replacing all NA's with col average
imdb$num_critic_for_reviews[is.na(imdb$num_critic_for_reviews)] <- round(mean(imdb$num_critic_for_reviews, na.rm = TRUE))
imdb$duration[is.na(imdb$duration)] <- round(mean(imdb$duration, na.rm = TRUE))
imdb$director_facebook_likes[is.na(imdb$director_facebook_likes)] <- round(mean(imdb$director_facebook_likes, na.rm = TRUE))
imdb$actor_3_facebook_likes[is.na(imdb$actor_3_facebook_likes)] <- round(mean(imdb$actor_3_facebook_likes, na.rm = TRUE))
imdb$actor_1_facebook_likes[is.na(imdb$actor_1_facebook_likes)] <- round(mean(imdb$actor_1_facebook_likes, na.rm = TRUE))
imdb$cast_total_facebook_likes[is.na(imdb$cast_total_facebook_likes)] <- round(mean(imdb$cast_total_facebook_likes, na.rm = TRUE))
imdb$actor_2_facebook_likes[is.na(imdb$actor_2_facebook_likes)] <- round(mean(imdb$actor_2_facebook_likes, na.rm = TRUE))
imdb$movie_facebook_likes[is.na(imdb$movie_facebook_likes)] <- round(mean(imdb$movie_facebook_likes, na.rm = TRUE))
# delete the blank cols in content rating as they cannot be replaced with anything reasonable
imdb <- imdb[!(imdb$content_rating %in% ""),]
view(imdb)
# replacing all content_rating with mordern rating system
imdb$content_rating[imdb$content_rating == 'M']   <- 'PG'
imdb$content_rating[imdb$content_rating == 'GP']  <- 'PG'
imdb$content_rating[imdb$content_rating == 'X']   <- 'NC-17'
imdb$content_rating[imdb$content_rating == 'Approved']  <- 'R'
imdb$content_rating[imdb$content_rating == 'Not Rated'] <- 'R'
imdb$content_rating[imdb$content_rating == 'Passed']    <- 'R'
imdb$content_rating[imdb$content_rating == 'Unrated']   <- 'R'
imdb$content_rating <- factor(imdb$content_rating)
levels(imdb$country) <- c(levels(imdb$country), "Others")
imdb$country[(imdb$country != 'USA')&(imdb$country != 'UK')] <- 'Others'
imdb$country <- factor(imdb$country)
table(imdb$country)
# Data Visualizing
## Add legend if needed
imdb %>%
filter(title_year %in% c(2000:2016)) %>%
arrange(desc(net_profit)) %>%
top_n(10, net_profit) %>%
ggplot(aes(x=budget/1000000, y=net_profit/1000000)) +
geom_point() +
geom_smooth() +
geom_text_repel(aes(label=movie_title)) +
labs(x = "Budget in million", y = "Profit in million", title = "Budget of the top 10 profitable movies between 2000 - 2016") +
theme(plot.title = element_text(hjust = 0.2))
# facet wrap
# change the budget x=budget/1000000
library(reshape2)
library(corrgram)
ggplot(data = melt(imdb), mapping = aes(x = value)) +
geom_histogram(bins = 10) + facet_wrap(~variable, scales = 'free_x')
# imdb score count
ggplot(imdb, aes(x= imdb_score)) + geom_bar()
#Akhila
#
imdb <- imdb[!is.na(imdb$num_user_for_reviews),]
imdb$num_user_reviews<-cut(imdb$num_user_for_reviews,breaks = c(0,107,208,333,397,5100), labels = c("very few","few","middle","high","very high"))
summary(imdb$num_user_reviews)
ggplot(imdb, aes(x =duration, y =imdb_score))+
geom_point(size=2, aes(colour=num_user_reviews)) +
labs(title = "Duration Vs. IMDB Score and Number of User Reviews",
x = "Duration", y = "IMDB Score")
# The higher the duration, the more the rating.
# Looks like duration of the movie also factors into the ratings of the movie.
# 2nd
ggplot(aes(x = title_year, y = imdb_score), data = imdb) +
geom_point() + geom_smooth(method = "auto")
# Older movies have a higher rating compared to new movies.
ggplot(imdb, aes(x =duration, y =imdb_score))+
geom_point(size=2, aes(colour=num_user_reviews)) +
labs(title = "Duration Vs. IMDB Score and Number of User Reviews",
x = "Duration", y = "IMDB Score")
ggplot(imdb, aes(x =duration < 150, y =imdb_score))+
geom_point(size=2, aes(colour=num_user_reviews)) +
labs(title = "Duration Vs. IMDB Score and Number of User Reviews",
x = "Duration", y = "IMDB Score")
ggplot(imdb, aes(x =duration, y =imdb_score))+
geom_point(size=2, aes(colour=num_user_reviews)) +
labs(title = "Duration Vs. IMDB Score and Number of User Reviews",
x = "Duration", y = "IMDB Score")
ggplot(imdb, aes(x =imdb_score, y =duration))+
geom_point(size=2, aes(colour=num_user_reviews)) +
labs(title = "Duration Vs. IMDB Score and Number of User Reviews",
x = "Duration", y = "IMDB Score")
ggplot(imdb, aes(x =imdb_score, y =duration))+
geom_point(size=2, aes(colour=num_user_reviews)) +
labs(title = "Movie Duration comapred to the IMDB score",
x = "IMDB Score", y = "Duration")
# 2nd
ggplot(aes(x = title_year, y = imdb_score), data = imdb) +
geom_point() + geom_smooth(method = "auto")
# 2nd
ggplot(aes(x = title_year, y = imdb_score), data = imdb) +
geom_point(size=2, aes(colour=num_user_reviews)) + geom_smooth(method = "auto")
# 2nd
ggplot(aes(x = title_year, y = imdb_score), data = imdb) +
geom_point(size=2, aes(colour=num_user_reviews),(alpha = 0.05)) + geom_smooth(method = "auto")
# 2nd
ggplot(aes(x = title_year, y = imdb_score), data = imdb) +
geom_point(size=2, aes(colour=num_user_reviews,(alpha = 0.05))) + geom_smooth(method = "auto")
# 2nd
ggplot(aes(x = title_year, y = imdb_score), data = imdb) +
geom_point(alpha = 1/10)) + geom_smooth(method = "auto")
# 2nd
ggplot(aes(x = title_year, y = imdb_score), data = imdb) +
geom_point(alpha = 1/10) + geom_smooth(method = "auto")
library(tree)
library(rpart)
library(rpart.plot)
install.packages("tree")
library(rpart.plot)
install.packages("rpart")
install.packages("rpart")
install.packages("rpart")
install.packages("rpart.plot")
library(tree)
library(rpart)
library(rpart.plot)
imdb_train_indices <- sample(1:nrow(imdb),0.8*nrow(imdb))
imdb_train <- imdb %>% slice(imdb_train_indices)
imdb_test <- imdb %>% slice(-imdb_train_indices)
library(tidyverse)
imdb_train <- imdb %>% slice(imdb_train_indices)
imdb_test <- imdb %>% slice(-imdb_train_indices)
imdb_mod_1 = lm(imdb_score~num_voted_users+duration,data=imdb_train)
summary(imdb_mod_1)
predict(imdb_mod_1,imdb_test)
imdb_rpart <- rpart(imdb_score~.,data=temp_train)
imdb_rpart <- rpart(imdb_score~.,data=imdb_train)
imdb_rpart <- rpart(imdb_score~.,data=imdb_train)
imdb_rpart
# R-squared for this model is 0.2797 which is extremely poor,
# this shows that imdb score is not highly corelated to number of votes or duration
# there is not a linenar relationship among the imdb score ~ num_user_review and duration
set.seed(3)
imdb_rpart <- rpart(imdb_score~.,data=imdb_train)
imdb_rpart
rpart.plot(imdb_rpart,digits = 3)
summary(imdb_rpart)
knitr::opts_chunk$set(echo = TRUE , dev = "png", dpi=300)
options(digits = 2)
library(readr)
library(dplyr)
library(ggplot2)
library(tidyverse)
library(stringr)
library(ggrepel)
knitr::opts_chunk$set(echo = TRUE , dev = "png", dpi=300)
options(digits = 2)
library(readr)
library(dplyr)
library(ggplot2)
library(tidyverse)
library(stringr)
library(ggrepel)
library(readr)
library(dplyr)
library(ggplot2)
library(tidyverse)
library(stringr)
library(ggrepel)
library(reshape2)
library(corrgram)
library(tree)
library(rpart)
library(rpart.plot)
library(randomForest)
library(plotly)
library(stringr)
source('D:/Study/Nafis-MS/Masters Study Material/1.Fall-2020/2.STA-9750/Project/Practise.R', echo=TRUE)
install.packages("plotly")
library(readr)
library(dplyr)
library(ggplot2)
library(tidyverse)
library(stringr)
library(ggrepel)
dim(imdb)
colSums(sapply(imdb, is.na))
imdb$movie_title <- gsub("Ã", "", as.character(factor(imdb$movie_title)))
str_trim(imdb$movie_title, side = "right")
imdb <- imdb[!is.na(imdb$gross), ]
imdb <- imdb[!is.na(imdb$budget), ]
imdb <- subset(imdb, select = -c(aspect_ratio))
imdb <- subset(imdb, select = -c(color))
imdb <- subset(imdb, select = -c(aspect_ratio))
imdb <- subset(imdb, select = -c(color))
imdb <- IMDB[!duplicated(IMDB), ]
imdb <- subset(imdb, select = -c(language))
imdb.directors <- data.frame (imdb %>%
group_by(director_name) %>%
summarise(count = n())%>%
filter(count >10, count <50))
ggplot(aes(x = director_name, y = count), data = imdb.directors)+
geom_jitter() +
theme(axis.text.x=element_text(angle=90,hjust=1,vjust=0.5)) +
labs(title="Directors Distribution on Movies",x="Count of Movies",y="Directors")
ggplot(aes(x = content_rating), data= subset(imdb, !is.na(content_rating))) +
geom_bar(aes(fill = ..count..)) +
theme(axis.text.x=element_text(angle=90,hjust=1,vjust=0.5)) +
labs(title="Content Rating Distribution ",x="Count of Movies",y="Content Rating")
imdb %>%
filter(title_year %in% c(2000:2016)) %>%
arrange(desc(net_profit)) %>%
top_n(10, net_profit) %>%
ggplot(aes(x=budget/1000000, y=net_profit/1000000)) +
geom_point() +
geom_smooth() +
geom_text_repel(aes(label=movie_title)) +
labs(x = "Budget in million", y = "Profit in million", title = "Budget of the top 10 profitable movies between 2000 - 2016") +
theme(plot.title = element_text(hjust = 0.2))
imdb %>%
filter(title_year %in% c(2000:2016)) %>%
arrange(desc(net_profit)) %>%
top_n(10, net_profit) %>%
ggplot(aes(x=budget/1000000, y=net_profit/1000000)) +
geom_point() +
geom_smooth() +
geom_text_repel(aes(label=movie_title)) +
labs(x = "Budget in million", y = "Profit in million", title = "Budget of the top 10 profitable movies between 2000 - 2016") +
theme(plot.title = element_text(hjust = 0.2))
ggplot(data = melt(imdb), mapping = aes(x = value)) +
geom_histogram(bins = 10) + facet_wrap(~variable, scales = 'free_x')
ggplot(aes(x=imdb_score), data = imdb) +
geom_histogram(binwidth = 0.2,aes(fill = ..count..)) +
scale_x_continuous(breaks = 0:10) +
ggtitle("Imdb Score Distribution") +
labs(x = "IMDB Score", y = "Count of Movies")
imdb.directors <- data.frame (imdb %>%
group_by(director_name) %>%
summarise(count = n())%>%
filter(count >10, count <50))
ggplot(aes(x = director_name, y = count), data = imdb.directors)+
geom_jitter() +
theme(axis.text.x=element_text(angle=90,hjust=1,vjust=0.5)) +
labs(title="Directors Distribution on Movies",x="Count of Movies",y="Directors")
imdb_rf <-  randomForest(imdb_score~ num_voted_users + duration + content_rating + genres +
num_critic_for_reviews, data=imdb_train,
ntree = 200, importance = TRUE, do.trace = 10)
library(readr)
library(dplyr)
library(ggplot2)
library(tidyverse)
library(stringr)
library(ggrepel)
library(reshape2)
library(corrgram)
library(tree)
library(rpart)
library(rpart.plot)
library(randomForest)
library(plotly)
knitr::opts_chunk$set(echo = TRUE , dev = "png", dpi=300)
options(digits = 2)
library(readr)
library(dplyr)
library(ggplot2)
library(tidyverse)
library(stringr)
library(ggrepel)
library(reshape2)
library(corrgram)
library(tree)
library(rpart)
library(rpart.plot)
library(randomForest)
library(plotly)
imdb %>% group_by(imdb_score) %>% filter(imdb_score > 7.5) %>% count(imdb_score) %>% arrange(desc(n))
ggplot(aes(x = imdb_score, y=net_profit/1000000 ), data = subset(imdb, net_profit > 1, !is.na(net_profit))) +
geom_jitter() +
geom_smooth() +
labs(title = "Net Profit and IMDB score",
x = " Imdb Score", y = "Net Profit in $ Million")
ggplot(aes(x=imdb_score), data = imdb) +
geom_histogram(binwidth = 0.2,aes(fill = ..count..)) +
scale_x_continuous(breaks = 0:10) +
ggtitle("Imdb Score Distribution") +
labs(x = "IMDB Score", y = "Count of Movies")
ggplot(aes(x=imdb_score), data = imdb) +
geom_histogram(binwidth = 0.2,aes(fill = ..count..)) +
scale_x_continuous(breaks = 0:10) +
ggtitle("Imdb Score Distribution") +
labs(x = "IMDB Score", y = "Count of Movies")
imdb$imdb_score
rating_imdb <- imdb %>% group_by(content_rating) %>%
filter(!is.na(content_rating)) %>%
summarise(average_imdb_score = mean(imdb_score), num = n())
ggplot(aes(x=average_imdb_score, y = num, label = content_rating),data = rating_imdb) +
geom_point() +
geom_label_repel()
rating_imdb <- imdb %>% group_by(content_rating) %>%
filter(!is.na(content_rating)) %>%
summarise(average_imdb_score = mean(imdb_score), num = n())
ggplot(aes(x=average_imdb_score, y = num, label = content_rating),data = rating_imdb) +
geom_point() +
geom_label_repel()
rating_imdb <- imdb %>% group_by(content_rating) %>%
filter(!is.na(content_rating)) %>%
summarise(average_imdb_score = mean(imdb_score), num = n())
ggplot(aes(x=average_imdb_score, y = num, label = content_rating),data = rating_imdb) +
geom_point() +
geom_label_repel()
imdb.directors <- data.frame (imdb %>%
group_by(director_name) %>%
summarise(count = n())%>%
filter(count >10, count <50))
ggplot(aes(x = director_name, y = count), data = imdb.directors)+
geom_jitter() +
theme(axis.text.x=element_text(angle=90,hjust=1,vjust=0.5)) +
labs(title="Directors Distribution on Movies",x="Directors",y="Count of Movies")
